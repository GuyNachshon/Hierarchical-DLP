# HRM-DLP Training Configuration
# Based on original HRM methodology adapted for DLP tasks

# Data configuration
train_path: "data/dlp_ag/train_split.jsonl"
val_path: "data/dlp_ag/val_split.jsonl"
max_length: 1024
tokenizer_path: null  # Use simple tokenizer

# Model architecture (following HRM paper)
vocab_size: 16000
hidden_size: 384
num_heads: 6
H_layers: 4      # High-level (slow) reasoning layers
L_layers: 4      # Low-level (fast) reasoning layers  
H_cycles: 2      # High-level reasoning cycles
L_cycles: 2      # Low-level reasoning cycles
expansion: 4.0   # MLP expansion factor

# ACT (Adaptive Compute Time) settings
use_act: true
act_max_steps: 4
act_exploration_prob: 0.0  # No exploration during initial training

# Training parameters (following HRM methodology)
epochs: 5
per_device_batch_size: 8
gradient_accumulation_steps: 8  # Effective batch size: 8 * 8 = 64
learning_rate: 3e-4
weight_decay: 0.02
beta1: 0.9
beta2: 0.95

# Learning rate scheduling
lr_warmup_steps: 1000
lr_min_ratio: 0.1

# Multi-task loss weights
doc_loss_weight: 1.0
span_loss_weight: 1.0
act_ponder_weight: 0.1
act_q_weight: 0.5

# Mixed precision
use_amp: true
amp_dtype: "bfloat16"

# Logging and evaluation
log_every_n_steps: 25
eval_every_n_steps: 200
save_every_n_steps: 500

# Experiment tracking
project_name: "hrm-dlp-production"
run_name: null  # Auto-generated

# Output
checkpoint_dir: "checkpoints/hrm_dlp_training"

# System
seed: 42